{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect and compare LLMs\n",
    "\n",
    "- Connect to the grok API and choose a model\n",
    "- Connect to the Gemini API and choose a model\n",
    "- Connect to the OpenAI API and choose 4o-mini\n",
    "\n",
    "### Create a prompt and inject a little text snippet of your liking\n",
    "- The LLM should use the injected information to answer a question.\n",
    "\n",
    "### Compare the outputs\n",
    "- Use the same method for every model.\n",
    "- Do you see differences?\n",
    "\n",
    "**Tipp:** for prompt injection you can either use string concatenation or the python String formatter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google.generativeai in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.8.5)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.6.15 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google.generativeai) (0.6.15)\n",
      "Requirement already satisfied: google-api-core in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google.generativeai) (2.25.0rc1)\n",
      "Requirement already satisfied: google-api-python-client in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google.generativeai) (2.169.0)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google.generativeai) (2.40.1)\n",
      "Requirement already satisfied: protobuf in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google.generativeai) (5.29.4)\n",
      "Requirement already satisfied: pydantic in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google.generativeai) (2.11.4)\n",
      "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google.generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in /home/codespace/.local/lib/python3.12/site-packages (from google.generativeai) (4.12.2)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-ai-generativelanguage==0.6.15->google.generativeai) (1.26.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-api-core->google.generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /home/codespace/.local/lib/python3.12/site-packages (from google-api-core->google.generativeai) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth>=2.15.0->google.generativeai) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth>=2.15.0->google.generativeai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth>=2.15.0->google.generativeai) (4.9.1)\n",
      "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-api-python-client->google.generativeai) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-api-python-client->google.generativeai) (0.2.0)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-api-python-client->google.generativeai) (4.1.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic->google.generativeai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic->google.generativeai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic->google.generativeai) (0.4.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.71.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google.generativeai) (1.71.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /home/codespace/.local/lib/python3.12/site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google.generativeai) (3.2.1)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google.generativeai) (0.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google.generativeai) (2025.1.31)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: openai in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.78.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (0.9.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /home/codespace/.local/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: groq in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.24.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from groq) (2.11.4)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in /home/codespace/.local/lib/python3.12/site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1,>=0.23.0->groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->groq) (0.4.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install google.generativeai\n",
    "! pip install openai\n",
    "! pip install groq\n",
    "! pip install dotenv\n",
    "import google.generativeai as genai\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "# Access the API key using the variable name defined in the .env file\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\"env/.env\")  # ← wichtig!\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001 - ['generateMessage', 'countMessageTokens']\n",
      "models/text-bison-001 - ['generateText', 'countTextTokens', 'createTunedTextModel']\n",
      "models/embedding-gecko-001 - ['embedText', 'countTextTokens']\n",
      "models/gemini-1.0-pro-vision-latest - ['generateContent', 'countTokens']\n",
      "models/gemini-pro-vision - ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-latest - ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-pro-001 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro-002 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-pro - ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-latest - ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-001 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-001-tuning - ['generateContent', 'countTokens', 'createTunedModel']\n",
      "models/gemini-1.5-flash - ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-002 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-1.5-flash-8b - ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-001 - ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-latest - ['createCachedContent', 'generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0827 - ['generateContent', 'countTokens']\n",
      "models/gemini-1.5-flash-8b-exp-0924 - ['generateContent', 'countTokens']\n",
      "models/gemini-2.5-pro-exp-03-25 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.5-pro-preview-03-25 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.5-flash-preview-04-17 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.5-flash-preview-04-17-thinking - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.5-pro-preview-05-06 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-exp - ['generateContent', 'countTokens', 'bidiGenerateContent']\n",
      "models/gemini-2.0-flash - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-001 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-lite-001 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-lite - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-lite-preview-02-05 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-lite-preview - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-pro-exp - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-pro-exp-02-05 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-exp-1206 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-thinking-exp-01-21 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-thinking-exp - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/gemini-2.0-flash-thinking-exp-1219 - ['generateContent', 'countTokens', 'createCachedContent']\n",
      "models/learnlm-2.0-flash-experimental - ['generateContent', 'countTokens']\n",
      "models/gemma-3-1b-it - ['generateContent', 'countTokens']\n",
      "models/gemma-3-4b-it - ['generateContent', 'countTokens']\n",
      "models/gemma-3-12b-it - ['generateContent', 'countTokens']\n",
      "models/gemma-3-27b-it - ['generateContent', 'countTokens']\n",
      "models/embedding-001 - ['embedContent']\n",
      "models/text-embedding-004 - ['embedContent']\n",
      "models/gemini-embedding-exp-03-07 - ['embedContent', 'countTextTokens']\n",
      "models/gemini-embedding-exp - ['embedContent', 'countTextTokens']\n",
      "models/aqa - ['generateAnswer']\n",
      "models/imagen-3.0-generate-002 - ['predict']\n",
      "models/veo-2.0-generate-001 - ['predictLongRunning']\n",
      "models/gemini-2.0-flash-live-001 - ['bidiGenerateContent', 'countTokens']\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "models = genai.list_models()\n",
    "for model in models:\n",
    "    print(model.name, \"-\", model.supported_generation_methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google\n",
    "\n",
    "https://ai.google.dev/gemini-api/docs/quickstart?hl=de&lang=python\n",
    "examples: https://colab.research.google.com/github/google-gemini/cookbook/blob/main/quickstarts/System_instructions.ipynb?hl=de#scrollTo=WxiIfsbA0WdH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\"env/.env\")\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "genai.configure(api_key=google_api_key)\n",
    "\n",
    "model = genai.GenerativeModel(\"models/gemini-1.5-pro\")  # ← wichtig: vollständiger Name\n",
    "response = model.generate_content(\"What is Retrieval-Augmented Generation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-Augmented Generation (RAG) is a technique in natural language processing (NLP) that combines the strengths of information retrieval (IR) systems with the generative capabilities of large language models (LLMs).  Instead of relying solely on the knowledge encoded within the LLM's parameters, RAG allows the model to access and process external information relevant to a given prompt, leading to more accurate, up-to-date, and comprehensive responses.\n",
      "\n",
      "Here's a breakdown of how it works:\n",
      "\n",
      "1. **Retrieval:** Given a user prompt, a relevant document or set of documents is retrieved from an external knowledge base. This knowledge base can be anything from a specific dataset, a collection of web pages, or even a codebase.  The retrieval process typically involves techniques like keyword search, semantic search, or embedding-based similarity matching.\n",
      "\n",
      "2. **Augmentation:**  The retrieved document(s) are then used to augment the prompt provided to the LLM. This can be done in several ways, such as:\n",
      "    * **Prepending retrieved documents to the prompt:** The LLM receives the retrieved information before the user's query, allowing it to consider the context before generating a response.\n",
      "    * **Inserting relevant excerpts within the prompt:** Specific parts of the retrieved documents most relevant to the query are inserted into the prompt.\n",
      "    * **Providing retrieved documents as separate context:** The retrieved documents are passed alongside the prompt, often with clear markers to distinguish between user input and retrieved context.\n",
      "\n",
      "3. **Generation:**  The LLM uses both the original prompt and the retrieved information to generate the output.  By grounding its response in the provided context, the LLM is less likely to hallucinate or generate factually incorrect information.\n",
      "\n",
      "**Benefits of RAG:**\n",
      "\n",
      "* **Improved Accuracy:** Access to external information allows LLMs to produce more accurate and factually grounded responses.\n",
      "* **Up-to-date Information:**  RAG enables LLMs to access the latest information, overcoming the limitations of their static training data.\n",
      "* **Reduced Hallucinations:** Providing relevant context minimizes the tendency of LLMs to generate fabricated or irrelevant information.\n",
      "* **Explainability and Traceability:**  By referencing the retrieved documents, RAG systems offer greater transparency into how the LLM arrived at its conclusions.\n",
      "* **Adaptability to Specific Domains:**  RAG can be easily adapted to specific domains by using specialized knowledge bases.\n",
      "\n",
      "\n",
      "**Examples of RAG Applications:**\n",
      "\n",
      "* **Question answering systems:** Providing accurate answers to user queries based on a specific knowledge base.\n",
      "* **Chatbots:** Enabling more informative and context-aware conversations.\n",
      "* **Text summarization:** Generating summaries that incorporate information from multiple sources.\n",
      "* **Code generation:** Assisting developers by retrieving relevant code snippets and documentation.\n",
      "\n",
      "**Challenges of RAG:**\n",
      "\n",
      "* **Retrieval Effectiveness:**  The quality of the retrieved documents directly impacts the quality of the generated output.  Improving retrieval mechanisms is crucial.\n",
      "* **Context Window Limitations:**  LLMs have limited context windows, restricting the amount of retrieved information they can process simultaneously.\n",
      "* **Computational Cost:** Retrieving and processing external information can be computationally expensive.\n",
      "* **Handling Noisy or Irrelevant Data:** The retrieved information may contain noisy or irrelevant data, which can negatively affect the LLM's performance.\n",
      "\n",
      "\n",
      "Despite these challenges, RAG represents a significant advancement in NLP, bridging the gap between information retrieval and generation to create more powerful and knowledgeable language models.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path=\"env/.env\")\n",
    "client = openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",  # Oder \"gpt-3.5-turbo\"\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the benefits of using retrieval-augmented generation?\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-augmented generation (RAG) combines retrieval techniques with generative models to produce more accurate and contextually relevant outputs. Here are the benefits of using RAG:\n",
      "\n",
      "1. **Improved Relevance**: By integrating retrieval mechanisms, RAG leverages external knowledge bases to enhance the contextual relevance of responses. This reduces the risk of generating inaccurate or outdated information, as the model can access up-to-date or domain-specific data.\n",
      "\n",
      "2. **Increased Accuracy**: By accessing a large body of documents or datasets during generation, RAG models can produce more factually accurate results. This is particularly valuable for tasks requiring precise information or real-time data.\n",
      "\n",
      "3. **Reduced Memory Load**: Generative models, on their own, require large memory and computational resources to store extensive knowledge. RAG shifts part of this requirement to external retrieval systems, making the overall architecture more efficient and scalable.\n",
      "\n",
      "4. **Enhanced Flexibility**: RAG models can be designed to access various and potentially evolving data sources, allowing for adaptable responses that can easily incorporate novel information without the need for retraining.\n",
      "\n",
      "5. **Domain Adaptability**: RAG enables easier adaptation to specific domains by accessing domain-specific databases or document corpora, allowing for increased specialization without requiring extensive retraining of the generative model itself.\n",
      "\n",
      "6. **Contextual Awareness**: With the ability to retrieve pertinent documents or passages, RAG ensures that generated content considers recent or context-important information, improving coherence and relevance in dynamic environments.\n",
      "\n",
      "7. **Cost-Effectiveness**: By retrieving specific information rather than generating all responses from scratch, RAG can reduce the computational overhead and time associated with generating complex responses from large-scale models.\n",
      "\n",
      "8. **Supporting Complex Queries**: Retrieval capabilities enhance the model's ability to handle complex questions by fetching related background information, thereby improving the completeness and informativeness of the generated answers.\n",
      "\n",
      "Overall, RAG represents a powerful approach by combining the strengths of retrieval systems and generative models to produce more accurate, efficient, and contextually aligned outputs.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq\n",
    "https://console.groq.com/docs/quickstart\n",
    "\n",
    "goal: llama-3.3-70b-versatile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval-augmented generation (RAG) is a type of language model that combines the strengths of traditional sequence-to-sequence models with the capabilities of retrieval-based models. While RAG has shown promising results in various natural language processing tasks, it also has some key limitations. Here are some of the main limitations of RAG:\n",
      "\n",
      "1. **Dependence on stored knowledge**: RAG relies heavily on the quality and diversity of the stored knowledge base. If the knowledge base is limited or biased, it can negatively impact the performance of the model.\n",
      "2. **Oversmoothing**: RAG can suffer from oversmoothing, where the generated text becomes too similar to the retrieved text, resulting in a loss of creativity and diversity.\n",
      "3. ** Limited contextual understanding**: RAG models may not fully understand the contextual nuances of the input text, which can lead to inaccuracies or incorrect outputs.\n",
      "4. **Inability to handle out-of-vocabulary (OOV) words**: RAG models can struggle with OOV words, which are not present in the training data. This can lead to errors or incomplete outputs.\n",
      "5. **Scalability issues**: As the size of the knowledge base grows, RAG models can become computationally expensive to train and evaluate.\n",
      "6. **Lack of interpretability**: RAG models can be challenging to interpret, as the generated text is influenced by both the generative and retrieval components.\n",
      "7. **Adversarial attacks**: RAG models can be vulnerable to adversarial attacks, such as injecting misleading information into the knowledge base.\n",
      "8. **Domain adaptation limitations**: RAG models may not generalize well to new domains or tasks, requiring additional training data or fine-tuning.\n",
      "9. **Language understanding limitations**: RAG models may not fully understand the nuances of human language, such as idioms, sarcasm, or figurative language.\n",
      "10. **Ethical concerns**: RAG models may perpetuate biases and stereotypes present in the training data, highlighting the need for careful consideration and evaluation of the ethical implications of these models.\n",
      "\n",
      "Despite these limitations, RAG models have shown promise in various applications, such as text summarization, question answering, and response generation. Researchers continue to work on addressing these limitations and developing more effective and scalable RAG models.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"env/.env\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "client = openai.OpenAI(\n",
    "    api_key=groq_api_key,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"  # wichtig: Groq verwendet die OpenAI-kompatible API\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama3-8b-8192\",  # Alternativ: \"llama3-70b-8192\" oder \"mixtral-8x7b-32768\"\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What are the key limitations of retrieval-augmented generation?\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
